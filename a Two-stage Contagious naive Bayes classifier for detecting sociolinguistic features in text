# Functions

1.   preprocessing - Simple pre-processing to remove noise from corpus
2.   features - Obtain feature (document x word) matrix for a set of input documents
3.   class_interest - Extract class specific feature matrix
4.   class_probability - Compute class prior
5.   likelihood_nb - Computes the MLE's for the multinomial naive Bayes classifier
6.   posterior_nb - Computes the posterior probabilities associated with the test scenarios
7.   NB_classification - Computes standard evaluation metrics such as accuracy, precision, recall and f1
8.   likelihood_cnb - Computes the MLE's for the contagious naive Bayes classifier
9.   posterior_cnb - Computes the posterior probabilities associated with the test scenarios
10.  CNB_classification - Computes standard evaluation metrics such as accuracy, precision, recall and f1

def preprocessing(file):
    '''
    input:
        file: Text documents
    output:
        lemmatized_out: List filled with preprocessed text
    description:
        Simple pre-processing to remove noise from corpus
    '''
    lemmatizer = WordNetLemmatizer()
    for row in file:
        sentence = str(row)
        # Set all characters to lowercase
        lower_case = sentence.lower()
        # Remove non-alphabetic characters
        no_alpha = re.sub(r'[^\w\s]','',lower_case)
        # Remove underscore character
        no_under = re.sub(r"_","",no_alpha)
        # Remove ccahracters longer than 15
        less_20 = re.sub("\w{15,}","",no_under)
        # tokenize
        token = word_tokenize(less_20)
        # lemmatize
        lemmatized_out = ' '.join([lemmatizer.lemmatize(w) for w in token])
        
        return lemmatized_out
    
def features(file):
    '''
    input:
        file: Text documents
    output:
        feature_mat: Bag of words (BoW) feature matrix
    description:
        Obtain feature (document x word) matrix for a set of input documents
    '''
    matrix = CountVectorizer(stop_words='english')
    feature_mat = matrix.fit_transform(file).toarray()
    feature_mat = pd.DataFrame(feature_mat)
    feature_names = matrix.get_feature_names()
    feature_mat.columns = feature_names
    
    return feature_mat

def class_interest(file, class_id):
    '''
    input:
        file: Bag of words feature matrix
        class_id: Integer that represents class of interest in class column
                    0: Negative
                    1: Positive
    output:
        class_f: Bag of words (BoW) for class of interest
    description:
        Extract class specific feature matrix
    '''
    class_f = file.loc[file['class_code'] == class_id]
    class_f = class_f.reset_index()
    class_f = class_f.drop(['class_code'], axis = 1)
    
    return class_f

def class_probability(file,class_id):
    '''
    input:
        file: Text documents
        class_id: Integer that represents class of interest in class column
                    0: Negative
                    1: Positive
    output:
        class_prob: Probability of class of interest 
    description:
        Compute class prior
    '''
    class_file = class_interest(file,class_id)
    class_count = len(class_file)
    total_count = len(file)
    class_prob = np.true_divide(class_count,total_count)
    
    return class_prob

#------------------------------Baseline Naive Bayes------------------------------#  
    
def likelihood_nb(file,class_id):
    '''
    input:
        file: Feature matrix of Text documents
        class_id: Integer that represents class of interest in class column
                    0: Negative
                    1: Positive
    output:
        likelihood: MLE's based on training scenarios
    description:
        Computes the MLE's for the multinomial naive Bayes classifier
    '''
    # likelihood
    class_file = class_interest(file,class_id)
    class_file = class_file.drop(['id_labels'],axis=1)
    feature_names = class_file.columns
    vocab_len = len(feature_names)
    sum_x_nij = []
    for n in class_file[feature_names]:
        sum_n = sum(class_file[n])
        sum_x_nij.append(sum_n)
        n_k = sum(sum_x_nij)
        denominator = np.add(n_k,vocab_len)
        numerator = np.add(sum_x_nij,1)
        fraction = np.true_divide(numerator,denominator)
        theta_hat = pd.DataFrame(fraction).transpose()
    theta_hat.columns = feature_names

    return theta_hat

def posterior_nb(test_file,train_features,class_id):
    '''
    input:
        test_file: Test case
        train_features: Feature matrix of text document (BoW)
        class_id: Integer that represents class of interest in class column
                    0: Negative
                    1: Positive
    output:
        posterior: Posterior probability of test case for class of interest
    description:
        Computes the posterior probabilities associated with the test scenarios
    '''
    c_hat = []
    test_features = features(test_file) # features present in test case
    train_nb = likelihood_nb(train_features,class_id) # mle for all features present in training case
    for j in range(len(test_features)):
        test_j_feature = test_features.loc[j] # doc_j in test set
        test_j_feature = test_j_feature.iloc[test_j_feature.nonzero()[0]] # features present in test case
        # class prior
        class_p = class_probability(train_features,class_id)
        class_p_log_space = np.log10(class_p)
        # conditional
        test_intersection = set(train_nb.columns).intersection(test_j_feature.index)
        class_mle = train_nb[test_intersection]
        conditional_log_space = np.log10(class_mle)
        # Decision rule
        c_hat_j = np.add(class_p_log_space,np.sum(conditional_log_space,axis=1))
        c_hat.append(c_hat_j)

    return(c_hat)
    
def NB_classification(feature_matrix,test_doc_clean):
    '''
    input:
        feature_matrix: Feature matrix of text document (BoW)
        test_doc_clean: Pre-processed test scenarios
    output:
        metric_frame: Standard classification metrics
    description:
        Computes standard evaluation metrics such as accuracy, precision, recall and f1
    '''
    class_code = np.unique(feature_matrix['class_code'])
    f = []
    for i in class_code:
        c_hat_test = posterior_nb(test_doc_clean,feature_matrix,i)
        for j in range(len(c_hat_test)):
            c_frame = (c_hat_test[j].values)
            frame = (i,j,c_frame[0])
            frame = list(frame)
            f.append(frame)
    
    c_hat_class = pd.DataFrame(f,columns = ['class_code','j','c_hat'])
    c_hat_doc = c_hat_class.groupby(['j']).agg(lambda x: list(x))
    c_hat_doc = c_hat_doc.reset_index()
    decision_doc_nb = c_hat_doc['c_hat']
    decision_doc_nb = pd.DataFrame(list(decision_doc_nb))
    decision_doc_nb['true_id'] = test_doc_clean.index
    
    decision_doc_nb['flag'] = (decision_doc_nb[0] < decision_doc_nb[1]).astype(int)
    decision_doc_nb = decision_doc_nb.set_index(decision_doc_nb['true_id'])
    decision_doc_nb['real'] = test_matrix['class_code'].astype(int)
    
    # Classification metrics
    acc_nb = accuracy_score(decision_doc_nb['real'], decision_doc_nb['flag'])
    precision_nb = precision_score(decision_doc_nb['real'], decision_doc_nb['flag'])
    recall_nb = recall_score(decision_doc_nb['real'], decision_doc_nb['flag'])
    f1_nb = f1_score(decision_doc_nb['real'], decision_doc_nb['flag'])
    
    metric_frame = (acc_nb,precision_nb,recall_nb,f1_nb)
    return metric_frame   
    
#------------------------------Contagious Naive Bayes------------------------------#   
    
def likelihood_cnb(file,class_id,c1,c2):
    '''
    input:
        file: Feature matrix of Text documents
        class_id: Integer that represents class of interest in class column
                    0: Negative
                    1: Positive
        c1: Smoothing parameter - numerator
        c2: Smoothing parameter - denominator
    output:
        likelihood: MLE's based on training scenarios
    description:
        Computes the MLE's for the contagious naive Bayes classifier
    '''
    class_file = class_interest(file,class_id)
    class_file = class_file.drop(['id_labels'],axis=1)
    feature_names = class_file.columns
    D_c = len(class_file)
    sum_x_ij = []
    for n in class_file[feature_names]:
        sum_n = sum(class_file[n])
        sum_x_ij.append(sum_n)
        n_k = list(sum_x_ij)
        #print(sum_x_ij,n_k)
        denominator = np.add(D_c,c2)
        numerator = np.add(n_k,c1)
        fraction = np.true_divide(numerator,denominator)
        
    theta_hat = pd.DataFrame(fraction).transpose()
    theta_hat.columns = feature_names
    
    return(theta_hat)

def posterior_cnb(test_file,train_features,class_id,c1,c2):
    '''
    input:
        test_file: Test case
        train_features: Feature matrix of text document (BoW)
        class_id: Integer that represents class of interest in class column
                    0: Negative
                    1: Positive
        c1: Smoothing parameter - numerator
        c2: Smoothing parameter - denominator
    output:
        posterior: Posterior probability of test scenarios for class of interest
    description:
        Computes the posterior probabilities associated with the test scenarios
    '''
    c_hat = []
    test_features = features(test_file) # features present in test case
    train_cnb = likelihood_cnb(train_features,class_id,c1,c2)
    for j in range(len(test_features)):
        test_j_feature = test_features.loc[j] # doc_j in test set
        test_j_feature = test_j_feature.iloc[test_j_feature.nonzero()[0]] # features present in test case
        # class prior
        class_p = class_probability(train_features,class_id)
        class_p_log_space = np.log10(class_p)
        # conditional
        test_intersection = set(train_cnb.columns).intersection(test_j_feature.index)
        class_mle = train_cnb[test_intersection]
        conditional_log_space = (np.log10(class_mle) - class_mle)
        # Decision rule
        c_hat_j = np.add(class_p_log_space,np.sum(conditional_log_space,axis=1))
        c_hat.append(c_hat_j)
        
    return(c_hat)
def CNB_classification(feature_matrix,test_doc_clean,c1,c2):
    '''
    input:
        feature_matrix: Feature matrix of text document (BoW)
        test_doc_clean: Pre-processed test scenarios
        c1: Smoothing parameter - numerator
        c2: Smoothing parameter - denominator
    output:
        metric_frame: Standard ckassification metrics
    description:
        Computes standard evaluation metrics such as accuracy, precision, recall and f1
    '''
    class_code = np.unique(feature_matrix['class_code'])
    f = []
    for i in class_code:
        c_hat_test = posterior_cnb(test_doc_clean,feature_matrix,i,c1,c2)
        for j in range(len(c_hat_test)):
            c_frame = (c_hat_test[j].values)
            frame = (i,j,c_frame[0])
            frame = list(frame)
            f.append(frame)
    
    c_hat_class = pd.DataFrame(f,columns = ['class_code','j','c_hat'])
    c_hat_doc = c_hat_class.groupby(['j']).agg(lambda x: list(x))
    c_hat_doc = c_hat_doc.reset_index()
    decision_doc_cnb = c_hat_doc['c_hat']
    decision_doc_cnb = pd.DataFrame(list(decision_doc_cnb))
    decision_doc_cnb['true_id'] = test_doc_clean.index
    
    decision_doc_cnb['flag'] = (decision_doc_cnb[0] < decision_doc_cnb[1]).astype(int)
    decision_doc_cnb = decision_doc_cnb.set_index(decision_doc_cnb['true_id'])
    decision_doc_cnb['real'] = test_matrix['class_code'].astype(int)

    # Classification metrics
    acc_cnb = accuracy_score(decision_doc_cnb['real'], decision_doc_cnb['flag'])
    precision_cnb = precision_score(decision_doc_cnb['real'], decision_doc_cnb['flag'])
    recall_cnb = recall_score(decision_doc_cnb['real'], decision_doc_cnb['flag'])
    f1_cnb = f1_score(decision_doc_cnb['real'], decision_doc_cnb['flag'])

    metric_frame = (acc_cnb,precision_cnb,recall_cnb,f1_cnb)
    return metric_frame
    
# Import Packages
import pandas as pd
import numpy as np

import re
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

import warnings
warnings.filterwarnings(action = 'ignore')

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

from bs4 import BeautifulSoup

# Data Preparation
# Import data
from google.colab import drive
drive.mount("/content/drive")
!ls "/content/drive/My Drive/Navorsing/Text Classification"
dataFile = pd.read_csv("/content/drive/My Drive/Navorsing/Text Classification/pan12-sexual-predator-identification-training-corpus-2012-05-01.csv")

# Import list of known predators
predator_ids = pd.read_csv("/content/drive/My Drive/Navorsing/Text Classification/pan12-sexual-predator-identification-training-corpus-predators-2012-05-01.txt", header = None)
predator_ids = predator_ids.values.tolist()
predator_ids = [id[0] for id in predator_ids]

message2sentence = []
for sentence in dataFile["message_content"]:
    no_apos = re.sub(r"&apos;","",str(sentence))
    no_amp = re.sub(r"&amp;","and",no_apos)
    no_quot = re.sub(r"&quot;","",no_amp)
    no_punct = re.sub(r"[;@?!$]+\ *",'',no_quot)
    no_dubble_space = re.sub("[ ]{2,}"," ",no_punct)
    no_under = re.sub(r"_","",no_dubble_space)
    no_http = re.sub(r"\b(?:(?:http?|ftp)://)?\w[\w-]*(?:\.[\w-]+)+\S*","",no_under)
    no_https = re.sub(r"\b(?:(?:https?|ftp)://)?\w[\w-]*(?:\.[\w-]+)+\S*","",no_http)
    no_html = BeautifulSoup(no_https,"html.parser").get_text()
    no_num = re.sub("\d+","",no_html) # Remove numerical characters
    less_20 = re.sub("\w{15,}","",no_num) # Remove words with more than 20 characters
    message2sentence.append(less_20)
    
dataFile["updated_content"] = (message2sentence)
conversation = dataFile.groupby("conversation_ID").agg(lambda x: list(x))
conversation = conversation.reset_index()   # reset index
# Get unique ids in each conversation
unique_id = [np.unique(ls) for ls in conversation['author_ID']]
conversation['unique_id'] = unique_id
predator = [True in [id in predator_ids for id in ids] for ids in conversation.unique_id]
conversation['label'] = np.array(predator, dtype = int)

# Due to memory constraints only take 2016 '0' and 2016 '1'
i_0 = (conversation['label'] == 0)
i_0 = conversation[i_0]
i_0 = i_0.sample(2016,random_state = 42) # 2016 because that's the most '1' we have, so we 'downsample' '0'
i_1 = (conversation['label'] == 1)
i_1 = conversation[i_1]

conv = pd.concat((i_0,i_1))
conv = conv.sample(frac = 1).reset_index(drop = True)
cols = ['updated_content','label']
dataFile_conv = conv[cols]
dataFile_conv.columns = ['text','class_code']

# Stage 1: Text Classification
## Classification based on 5-Fold
### 5-Fold settings:
1. random_state = None
2. shuffle = False

### Metrics used:
1. Accuracy
2. Precision
3. Recall
4. F1

dataFile_conv.columns = ['text','class_code']
kf = KFold(n_splits = 5, random_state = None, shuffle = False) 
kf.get_n_splits(dataFile_conv['text'])
nb_metrics = []
cnb_metrics = []
for train_index, test_index in kf.split(dataFile_conv['text']):
    X_train, X_test = dataFile_conv['text'][train_index], dataFile_conv['text'][test_index]
    y_train, y_test = dataFile_conv['class_code'][train_index], dataFile_conv['class_code'][test_index]
    train_matrix = (X_train,y_train)
    train_matrix = pd.DataFrame(train_matrix).transpose()
    train_matrix_id = train_matrix.index
    test_matrix = (X_test,y_test)
    test_matrix = pd.DataFrame(test_matrix).transpose()
    # Preprocessing of the corpus
    corpus_clean = train_matrix.apply(lambda x: preprocessing(x), axis = 1)
    
    # Obtain feature matrix for train data
    feature_matrix = features(corpus_clean)
    feature_matrix['id_labels'] = train_matrix_id
    feature_matrix = feature_matrix.set_index(feature_matrix['id_labels'])
    feature_matrix = feature_matrix.drop(columns = ['id_labels'])
    
    feature_matrix["class_code"] = train_matrix["class_code"] # Add class column to feature matrix to seperate classes
    test_matrix.columns = ['text','class_code']
    test_doc = pd.DataFrame(test_matrix)
    test_doc_clean = test_doc.apply(lambda x: preprocessing(x), axis = 1)
    print('features = done')
    # Naive Bayes
    x_nb = NB_classification(feature_matrix,test_doc_clean)
    nb_metrics.append(x_nb)
    print('NB = done')
    # Contagious Naive Bayes
    x_cnb = CNB_classification(feature_matrix,test_doc_clean,0.001,1)
    cnb_metrics.append(x_cnb)
    print('CNB = done')
    
## Classification based on Document Length Normalization
dataFile_conv.columns = ['text','class_code']    
kf = KFold(n_splits=5, random_state=None, shuffle=False) 
kf.get_n_splits(dataFile_conv['text'])
cnb_dln_metrics = []
for train_index, test_index in kf.split(dataFile_conv['text']):
    #print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = dataFile_conv['text'][train_index], dataFile_conv['text'][test_index]
    y_train, y_test = dataFile_conv['class_code'][train_index], dataFile_conv['class_code'][test_index]
    
    train_matrix = (X_train,y_train)
    train_matrix = pd.DataFrame(train_matrix).transpose()
    train_matrix_id = train_matrix.index
    test_matrix = (X_test,y_test)
    test_matrix = pd.DataFrame(test_matrix).transpose()

    # concatenate docs of same class together (from training set)
    corpus_clean = train_matrix.apply(lambda x: preprocessing(x), axis = 1)
    
    doc_new = pd.DataFrame(corpus_clean,columns = ['text'])
    doc_new['class_code'] = train_matrix['class_code'].astype(int)
    doc_new_id = doc_new.index
    
    doc_nc = doc_new.groupby(['class_code']).agg(lambda x: list(x))
    doc_nc = doc_nc.reset_index()
    cols = ['text','class_code']
    doc_nc = doc_nc[cols]
    doc_new_c = pd.DataFrame(doc_nc.apply(lambda x: preprocessing(x),axis=1),columns = ['text'])
    doc_new_c['class_code'] = doc_nc['class_code']
    
    class_codes = np.unique(doc_new_c['class_code'])
    df = pd.DataFrame(columns = ['class_code','text'])
    for i in class_codes:
        ci = class_interest(doc_new_c,i)
        for n in ci['text']:
            doc_len = len(n.split())
            pseudo_len = 124
            pseudo = round(doc_len / pseudo_len)
            pseudo_doc = [n.split()[k:k + pseudo_len] for k in range(0,doc_len,pseudo_len)]
            pseudo_doc = [' '.join(pseudo_doc[k]) for k in range(0,len(pseudo_doc))]
            print(len(pseudo_doc))
            pseudo_doc_class = ([i]*len(pseudo_doc))
            for j in range(0,len(pseudo_doc)):
                pseudo_jj = (pseudo_doc_class[j], pseudo_doc[j])
                pseudo_jj = list(pseudo_jj)
                pseudo_df = pd.Series(pseudo_jj, index = ["class_code","text"])
                df = df.append(pseudo_df, ignore_index = True)
    print('phase 1: Complete')            
    # Feature matrix for DLN data
    feature_matrix_DLN = features(df['text'])
    
    feature_matrix_DLN["class_code"] = df["class_code"] # Add class column to feature matrix to seperate classes
    feature_matrix_ids = feature_matrix_DLN.index
    feature_matrix_DLN['id_labels'] = feature_matrix_ids
    test_doc = pd.DataFrame(test_matrix)
    test_doc_clean = test_doc.apply(lambda x: preprocessing(x), axis = 1)
    
    # Contagious Naive Bayes
    x_dln_cnb = CNB_classification(feature_matrix_DLN,test_doc_clean,0.001,1)
    cnb_dln_metrics.append(x_dln_cnb)

    print('phase 2: Complete')
    print('######################################################################')

metric_cols = ['Accuracy','Precision','Recall','F1']
cnb_dln_m = pd.DataFrame(cnb_dln_metrics,columns = metric_cols)
cnb_dln_mean = cnb_dln_m.mean()
print(cnb_dln_mean)

# Stage 2: Extract Top 100 features based on highest MLE's of each class for Bayesian Network
### Obtain feature matrix for entire training corpus

dataFile_conv.columns = ['text','class_code']
X_train, X_test, y_train, y_test = train_test_split(dataFile_conv['text'], dataFile_conv['class_code'], test_size=0.2, random_state=42)

train_matrix = (X_train,y_train)
train_matrix = pd.DataFrame(train_matrix).transpose()
train_matrix_id = train_matrix.index
test_matrix = (X_test,y_test)
test_matrix = pd.DataFrame(test_matrix).transpose()
# Preprocessing of the corpus
corpus_clean = train_matrix.apply(lambda x: preprocessing(x), axis = 1)
    
# Obtain feature matrix for train data
feature_matrix = features(corpus_clean)
feature_matrix['id_labels'] = train_matrix_id
feature_matrix = feature_matrix.set_index(feature_matrix['id_labels'])
feature_matrix = feature_matrix.drop(columns = ['id_labels'])
    
feature_matrix["class_code"] = train_matrix["class_code"] # Add class column to feature matrix to seperate classes

## MLE's for unsupervised Bayesian network structure learning

Note: Set class_ids to either 0 or 1

### Baseline Naive Bayes
# Get likelihoods for unsupervised learning - NB
class_ids = 0
nb_likelihood = likelihood_nb(feature_matrix,class_ids)
nb_likelihood_cols = list(nb_likelihood.columns)
nb_likelihood_tp = nb_likelihood.transpose()
nb_likelihood_tp.columns = ['likelihood']
nb_likelihood_tp['word'] = nb_likelihood_cols

nb_top_100_likelihood = nb_likelihood_tp.nlargest(100,['likelihood'])
nb_top_100_likelihood = nb_top_100_likelihood.T
nb_top_100_likelihood = nb_top_100_likelihood.drop(['word'])
# Extract BoW for these top_100 observations
nb_cols = nb_top_100_likelihood.columns
nb_BoW = class_interest(feature_matrix,class_ids)
nb_BoW = nb_BoW[nb_cols]
nb_BoW.to_csv('PAN_top_100_nb.csv',index=False)

### Contagious Naive Bayes
# Get likelihoods for unsupervised learning - CNB
class_ids = 0
cnb_likelihood = likelihood_cnb(feature_matrix,class_ids,0.001,1)
cnb_likelihood_cols = list(cnb_likelihood.columns)
cnb_likelihood_tp = cnb_likelihood.transpose()
cnb_likelihood_tp.columns = ['likelihood']
cnb_likelihood_tp['word'] = cnb_likelihood_cols

cnb_top_100_likelihood = cnb_likelihood_tp.nlargest(100,['likelihood'])
cnb_top_100_likelihood = cnb_top_100_likelihood.T
cnb_top_100_likelihood = cnb_top_100_likelihood.drop(['word'])
# Extract BoW for these top_100 observations
cnb_cols = cnb_top_100_likelihood.columns
cnb_BoW = class_interest(feature_matrix,class_ids)
cnb_BoW = cnb_BoW[cnb_cols]
cnb_BoW.to_csv('PAN_top_100_cnb.csv',index=False)

## MLE's based on Document Length Normalization

1.   Document Length Normalization
2.   MLE calculation

### Document Length Normalization
dataFile_conv.columns = ['text','class_code']
X_train, X_test, y_train, y_test = train_test_split(dataFile_conv['text'], dataFile_conv['class_code'], test_size=0.2, random_state=42)

train_matrix = (X_train,y_train)
train_matrix = pd.DataFrame(train_matrix).transpose()
train_matrix_id = train_matrix.index
test_matrix = (X_test,y_test)
test_matrix = pd.DataFrame(test_matrix).transpose()
# Preprocessing of the corpus
corpus_clean = train_matrix.apply(lambda x: preprocessing(x), axis = 1)
    
doc_new = pd.DataFrame(corpus_clean,columns = ['text'])
doc_new['class_code'] = train_matrix['class_code'].astype(int)
doc_new_id = doc_new.index
    
doc_nc = doc_new.groupby(['class_code']).agg(lambda x: list(x))
doc_nc = doc_nc.reset_index()
cols = ['text','class_code']
doc_nc = doc_nc[cols]
doc_new_c = pd.DataFrame(doc_nc.apply(lambda x: preprocessing(x),axis=1),columns = ['text'])
doc_new_c['class_code'] = doc_nc['class_code']
    
class_codes = np.unique(doc_new_c['class_code'])
df = pd.DataFrame(columns = ['class_code','text'])
for i in class_codes:
  ci = class_interest(doc_new_c,i)
  for n in ci['text']:
    doc_len = len(n.split())
    pseudo_len = 124
    pseudo = round(doc_len / pseudo_len)
    pseudo_doc = [n.split()[k:k + pseudo_len] for k in range(0,doc_len,pseudo_len)]
    pseudo_doc = [' '.join(pseudo_doc[k]) for k in range(0,len(pseudo_doc))]
    print(len(pseudo_doc))
    pseudo_doc_class = ([i]*len(pseudo_doc))
    for j in range(0,len(pseudo_doc)):
      pseudo_jj = (pseudo_doc_class[j], pseudo_doc[j])
      pseudo_jj = list(pseudo_jj)
      pseudo_df = pd.Series(pseudo_jj, index = ["class_code","text"])
      df = df.append(pseudo_df, ignore_index = True)           
# Feature matrix for DLN data
feature_matrix_DLN = features(df['text'])
feature_matrix_DLN["class_code"] = df["class_code"] # Add class column to feature matrix to seperate classes
feature_matrix_ids = feature_matrix_DLN.index
feature_matrix_DLN['id_labels'] = feature_matrix_ids

### MLE calculations
# Get likelihoods for unsupervised learning - CNB
class_ids = 0
class_file = class_interest(feature_matrix_DLN,class_ids)
class_file = class_file.drop(['id_labels','index','level_0'],axis=1)
feature_names = class_file.columns
vocab_len = len(feature_names)
sum_x_nij = []
for n in class_file[feature_names]:
  sum_n = sum(class_file[n])
  sum_x_nij.append(sum_n)
  n_k = sum(sum_x_nij)
  denominator = np.add(n_k,vocab_len)
  numerator = np.add(sum_x_nij,1)
  fraction = np.true_divide(numerator,denominator)
  theta_hat = pd.DataFrame(fraction).transpose()
theta_hat.columns = feature_names
#cnb_likelihood_dln = likelihood_cnb(feature_matrix_DLN,0,0.001,1)
cnb_likelihood_dln = theta_hat
cnb_likelihood_dln_cols = list(cnb_likelihood_dln.columns)
cnb_likelihood_dln_tp = cnb_likelihood_dln.transpose()
cnb_likelihood_dln_tp.columns = ['likelihood']
cnb_likelihood_dln_tp['word'] = cnb_likelihood_dln_cols

cnb_top_100_likelihood_dln = cnb_likelihood_dln_tp.nlargest(100,['likelihood'])
cnb_top_100_likelihood_dln = cnb_top_100_likelihood_dln.T
cnb_top_100_likelihood_dln = cnb_top_100_likelihood_dln.drop(['word'])
# Extract BoW for these top_100 observations
cnb_cols_0 = cnb_top_100_likelihood_dln.columns
cnb_BoW = class_interest(feature_matrix_DLN,class_ids)
cnb_BoW = cnb_BoW[cnb_cols_0]
cnb_BoW.to_csv('PAN_top_100_dln_cnb0.csv',index=False)
